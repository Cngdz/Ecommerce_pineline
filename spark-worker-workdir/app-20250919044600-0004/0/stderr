Spark Executor Command: "/usr/lib/jvm/java-17-openjdk-amd64/bin/java" "-cp" "/opt/spark/conf/:/opt/spark/jars/*" "-Xmx1024M" "-Dspark.network.timeout=600000" "-Dspark.driver.port=40611" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@1acb2f36abb5:40611" "--executor-id" "0" "--hostname" "172.19.0.8" "--cores" "12" "--app-id" "app-20250919044600-0004" "--worker-url" "spark://Worker@172.19.0.8:43569" "--resourceProfileId" "0"
========================================

Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
25/09/19 04:46:02 INFO CoarseGrainedExecutorBackend: Started daemon with process name: 565@d9e2d2ad2224
25/09/19 04:46:02 INFO SignalUtils: Registering signal handler for TERM
25/09/19 04:46:02 INFO SignalUtils: Registering signal handler for HUP
25/09/19 04:46:02 INFO SignalUtils: Registering signal handler for INT
25/09/19 04:46:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/09/19 04:46:03 INFO SecurityManager: Changing view acls to: airflow
25/09/19 04:46:03 INFO SecurityManager: Changing modify acls to: airflow
25/09/19 04:46:03 INFO SecurityManager: Changing view acls groups to: 
25/09/19 04:46:03 INFO SecurityManager: Changing modify acls groups to: 
25/09/19 04:46:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY
25/09/19 04:46:03 INFO TransportClientFactory: Successfully created connection to 1acb2f36abb5/172.19.0.13:40611 after 89 ms (0 ms spent in bootstraps)
25/09/19 04:46:03 INFO SecurityManager: Changing view acls to: airflow
25/09/19 04:46:03 INFO SecurityManager: Changing modify acls to: airflow
25/09/19 04:46:03 INFO SecurityManager: Changing view acls groups to: 
25/09/19 04:46:03 INFO SecurityManager: Changing modify acls groups to: 
25/09/19 04:46:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY
25/09/19 04:46:03 INFO TransportClientFactory: Successfully created connection to 1acb2f36abb5/172.19.0.13:40611 after 3 ms (0 ms spent in bootstraps)
25/09/19 04:46:03 INFO DiskBlockManager: Created local directory at /tmp/spark-4de01272-8fe0-4486-a344-dbe143bda4f5/executor-7de3145d-3b21-464a-99be-0bac78900ac9/blockmgr-746e2856-e5b8-438c-9920-0843a4809efb
25/09/19 04:46:03 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
25/09/19 04:46:04 INFO CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@1acb2f36abb5:40611
25/09/19 04:46:04 INFO WorkerWatcher: Connecting to worker spark://Worker@172.19.0.8:43569
25/09/19 04:46:04 INFO TransportClientFactory: Successfully created connection to /172.19.0.8:43569 after 4 ms (0 ms spent in bootstraps)
25/09/19 04:46:04 INFO ResourceUtils: ==============================================================
25/09/19 04:46:04 INFO ResourceUtils: No custom resources configured for spark.executor.
25/09/19 04:46:04 INFO WorkerWatcher: Successfully connected to spark://Worker@172.19.0.8:43569
25/09/19 04:46:04 INFO ResourceUtils: ==============================================================
25/09/19 04:46:04 INFO CoarseGrainedExecutorBackend: Successfully registered with driver
25/09/19 04:46:04 INFO Executor: Starting executor ID 0 on host 172.19.0.8
25/09/19 04:46:04 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
25/09/19 04:46:04 INFO Executor: Java version 17.0.16
25/09/19 04:46:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33949.
25/09/19 04:46:04 INFO NettyBlockTransferService: Server created on 172.19.0.8:33949
25/09/19 04:46:04 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/09/19 04:46:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(0, 172.19.0.8, 33949, None)
25/09/19 04:46:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(0, 172.19.0.8, 33949, None)
25/09/19 04:46:04 INFO BlockManager: Initialized BlockManager: BlockManagerId(0, 172.19.0.8, 33949, None)
25/09/19 04:46:04 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
25/09/19 04:46:04 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@682871a2 for default.
25/09/19 04:46:06 INFO CoarseGrainedExecutorBackend: Got assigned task 0
25/09/19 04:46:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
25/09/19 04:46:06 INFO TorrentBroadcast: Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
25/09/19 04:46:06 INFO TransportClientFactory: Successfully created connection to 1acb2f36abb5/172.19.0.13:39619 after 2 ms (0 ms spent in bootstraps)
25/09/19 04:46:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.4 MiB)
25/09/19 04:46:06 INFO TorrentBroadcast: Reading broadcast variable 1 took 98 ms
25/09/19 04:46:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.6 KiB, free 434.4 MiB)
25/09/19 04:46:06 INFO FileScanRDD: Reading File path: s3a://bronze/raw_data/tiki_products_20250919_043608.json, range: 0-1185486, partition values: [empty row]
25/09/19 04:46:07 INFO CodeGenerator: Code generated in 162.333303 ms
25/09/19 04:46:07 INFO TorrentBroadcast: Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
25/09/19 04:46:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.3 KiB, free 434.3 MiB)
25/09/19 04:46:07 INFO TorrentBroadcast: Reading broadcast variable 0 took 13 ms
25/09/19 04:46:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 376.6 KiB, free 434.0 MiB)
25/09/19 04:46:07 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
25/09/19 04:46:07 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
25/09/19 04:46:07 INFO MetricsSystemImpl: s3a-file-system metrics system started
25/09/19 04:46:08 INFO MemoryStore: Block rdd_3_0 stored as values in memory (estimated size 291.5 KiB, free 433.7 MiB)
25/09/19 04:46:08 INFO Executor: 1 block locks were not released by task 0.0 in stage 0.0 (TID 0)
[rdd_3_0]
25/09/19 04:46:08 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1416 bytes result sent to driver
25/09/19 04:46:08 INFO CoarseGrainedExecutorBackend: Got assigned task 1
25/09/19 04:46:08 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
25/09/19 04:46:08 INFO TorrentBroadcast: Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
25/09/19 04:46:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.0 KiB, free 433.7 MiB)
25/09/19 04:46:08 INFO TorrentBroadcast: Reading broadcast variable 2 took 12 ms
25/09/19 04:46:08 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 32.5 KiB, free 433.7 MiB)
25/09/19 04:46:09 INFO BlockManager: Found block rdd_3_0 locally
25/09/19 04:46:09 INFO CodeGenerator: Code generated in 8.432686 ms
25/09/19 04:46:09 INFO CodeGenerator: Code generated in 36.28256 ms
25/09/19 04:46:09 INFO CodeGenerator: Code generated in 27.824909 ms
25/09/19 04:46:09 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2205 bytes result sent to driver
25/09/19 04:46:09 INFO CoarseGrainedExecutorBackend: Got assigned task 2
25/09/19 04:46:09 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
25/09/19 04:46:09 INFO MapOutputTrackerWorker: Updating epoch to 1 and clearing cache
25/09/19 04:46:09 INFO TorrentBroadcast: Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
25/09/19 04:46:09 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.7 MiB)
25/09/19 04:46:09 INFO TorrentBroadcast: Reading broadcast variable 3 took 17 ms
25/09/19 04:46:09 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.5 KiB, free 433.7 MiB)
25/09/19 04:46:09 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them
25/09/19 04:46:09 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@1acb2f36abb5:40611)
25/09/19 04:46:09 INFO MapOutputTrackerWorker: Got the map output locations
25/09/19 04:46:09 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
25/09/19 04:46:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
25/09/19 04:46:09 INFO CodeGenerator: Code generated in 17.498162 ms
25/09/19 04:46:09 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 4031 bytes result sent to driver
25/09/19 04:46:09 INFO CoarseGrainedExecutorBackend: Got assigned task 3
25/09/19 04:46:09 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
25/09/19 04:46:09 INFO TorrentBroadcast: Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
25/09/19 04:46:09 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 433.7 MiB)
25/09/19 04:46:09 INFO TorrentBroadcast: Reading broadcast variable 4 took 26 ms
25/09/19 04:46:09 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 37.6 KiB, free 433.7 MiB)
25/09/19 04:46:09 INFO BlockManager: Found block rdd_3_0 locally
25/09/19 04:46:09 INFO CodeGenerator: Code generated in 4.487807 ms
25/09/19 04:46:09 INFO CodeGenerator: Code generated in 28.339594 ms
25/09/19 04:46:10 INFO CodeGenerator: Code generated in 31.694066 ms
25/09/19 04:46:10 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 2205 bytes result sent to driver
25/09/19 04:46:10 INFO CoarseGrainedExecutorBackend: Got assigned task 4
25/09/19 04:46:10 INFO Executor: Running task 0.0 in stage 6.0 (TID 4)
25/09/19 04:46:10 INFO MapOutputTrackerWorker: Updating epoch to 2 and clearing cache
25/09/19 04:46:10 INFO TorrentBroadcast: Started reading broadcast variable 5 with 1 pieces (estimated total size 4.0 MiB)
25/09/19 04:46:10 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.7 MiB)
25/09/19 04:46:10 INFO TorrentBroadcast: Reading broadcast variable 5 took 26 ms
25/09/19 04:46:10 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.5 KiB, free 433.7 MiB)
25/09/19 04:46:10 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
25/09/19 04:46:10 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@1acb2f36abb5:40611)
25/09/19 04:46:10 INFO MapOutputTrackerWorker: Got the map output locations
25/09/19 04:46:10 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
25/09/19 04:46:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
25/09/19 04:46:10 INFO Executor: Finished task 0.0 in stage 6.0 (TID 4). 4031 bytes result sent to driver
25/09/19 04:46:10 INFO CoarseGrainedExecutorBackend: Got assigned task 5
25/09/19 04:46:10 INFO Executor: Running task 0.0 in stage 7.0 (TID 5)
25/09/19 04:46:10 INFO TorrentBroadcast: Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
25/09/19 04:46:10 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 14.9 KiB, free 433.7 MiB)
25/09/19 04:46:10 INFO TorrentBroadcast: Reading broadcast variable 6 took 16 ms
25/09/19 04:46:10 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 32.3 KiB, free 433.7 MiB)
25/09/19 04:46:10 INFO BlockManager: Found block rdd_3_0 locally
25/09/19 04:46:10 INFO CodeGenerator: Code generated in 7.725857 ms
25/09/19 04:46:10 INFO Executor: Finished task 0.0 in stage 7.0 (TID 5). 2162 bytes result sent to driver
25/09/19 04:46:10 INFO CoarseGrainedExecutorBackend: Got assigned task 6
25/09/19 04:46:10 INFO Executor: Running task 0.0 in stage 9.0 (TID 6)
25/09/19 04:46:10 INFO MapOutputTrackerWorker: Updating epoch to 3 and clearing cache
25/09/19 04:46:10 INFO TorrentBroadcast: Started reading broadcast variable 7 with 1 pieces (estimated total size 4.0 MiB)
25/09/19 04:46:10 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.7 MiB)
25/09/19 04:46:10 INFO TorrentBroadcast: Reading broadcast variable 7 took 12 ms
25/09/19 04:46:10 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 12.5 KiB, free 433.6 MiB)
25/09/19 04:46:10 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them
25/09/19 04:46:10 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@1acb2f36abb5:40611)
25/09/19 04:46:10 INFO MapOutputTrackerWorker: Got the map output locations
25/09/19 04:46:10 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
25/09/19 04:46:10 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
25/09/19 04:46:10 INFO Executor: Finished task 0.0 in stage 9.0 (TID 6). 3995 bytes result sent to driver
25/09/19 04:46:11 INFO CoarseGrainedExecutorBackend: Got assigned task 7
25/09/19 04:46:11 INFO Executor: Running task 0.0 in stage 10.0 (TID 7)
25/09/19 04:46:11 INFO TorrentBroadcast: Started reading broadcast variable 8 with 1 pieces (estimated total size 4.0 MiB)
25/09/19 04:46:12 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 433.7 MiB)
25/09/19 04:46:12 INFO TorrentBroadcast: Reading broadcast variable 8 took 15 ms
25/09/19 04:46:12 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 38.4 KiB, free 433.7 MiB)
25/09/19 04:46:12 INFO BlockManager: Found block rdd_3_0 locally
25/09/19 04:46:12 INFO CodeGenerator: Code generated in 28.560007 ms
25/09/19 04:46:12 INFO CodeGenerator: Code generated in 38.096981 ms
25/09/19 04:46:12 INFO Executor: Finished task 0.0 in stage 10.0 (TID 7). 2107 bytes result sent to driver
25/09/19 04:46:12 INFO CoarseGrainedExecutorBackend: Got assigned task 8
25/09/19 04:46:12 INFO Executor: Running task 0.0 in stage 12.0 (TID 8)
25/09/19 04:46:12 INFO MapOutputTrackerWorker: Updating epoch to 4 and clearing cache
25/09/19 04:46:12 INFO TorrentBroadcast: Started reading broadcast variable 10 with 1 pieces (estimated total size 4.0 MiB)
25/09/19 04:46:12 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 433.7 MiB)
25/09/19 04:46:12 INFO TorrentBroadcast: Reading broadcast variable 10 took 12 ms
25/09/19 04:46:12 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 10.7 KiB, free 433.6 MiB)
25/09/19 04:46:12 INFO MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them
25/09/19 04:46:12 INFO MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@1acb2f36abb5:40611)
25/09/19 04:46:12 INFO MapOutputTrackerWorker: Got the map output locations
25/09/19 04:46:12 INFO ShuffleBlockFetcherIterator: Getting 1 (194.1 KiB) non-empty blocks including 1 (194.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
25/09/19 04:46:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
25/09/19 04:46:12 INFO TorrentBroadcast: Started reading broadcast variable 9 with 1 pieces (estimated total size 4.0 MiB)
25/09/19 04:46:12 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 29.8 KiB, free 433.7 MiB)
25/09/19 04:46:12 INFO TorrentBroadcast: Reading broadcast variable 9 took 14 ms
25/09/19 04:46:12 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 32.0 KiB, free 433.6 MiB)
25/09/19 04:46:12 ERROR Executor: Exception in task 0.0 in stage 12.0 (TID 8)
org.apache.iceberg.exceptions.ValidationException: Cannot find source column for partition field: 1000: ingestion_date_day: day(10)
	at org.apache.iceberg.exceptions.ValidationException.check(ValidationException.java:49)
	at org.apache.iceberg.PartitionSpec.checkCompatibility(PartitionSpec.java:562)
	at org.apache.iceberg.PartitionSpec$Builder.build(PartitionSpec.java:543)
	at org.apache.iceberg.UnboundPartitionSpec.bind(UnboundPartitionSpec.java:46)
	at org.apache.iceberg.PartitionSpecParser.fromJson(PartitionSpecParser.java:71)
	at org.apache.iceberg.PartitionSpecParser.lambda$fromJson$1(PartitionSpecParser.java:88)
	at org.apache.iceberg.util.JsonUtil.parse(JsonUtil.java:98)
	at org.apache.iceberg.PartitionSpecParser.lambda$fromJson$2(PartitionSpecParser.java:88)
	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)
	at java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)
	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)
	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)
	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)
	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)
	at org.apache.iceberg.PartitionSpecParser.fromJson(PartitionSpecParser.java:86)
	at org.apache.iceberg.SerializableTable.lambda$specs$1(SerializableTable.java:196)
	at java.base/java.util.HashMap.forEach(HashMap.java:1421)
	at org.apache.iceberg.SerializableTable.specs(SerializableTable.java:194)
	at org.apache.iceberg.spark.source.SparkWrite$WriterFactory.createWriter(SparkWrite.java:674)
	at org.apache.iceberg.spark.source.SparkWrite$WriterFactory.createWriter(SparkWrite.java:668)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:441)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
25/09/19 04:46:12 INFO CoarseGrainedExecutorBackend: Got assigned task 9
25/09/19 04:46:12 INFO Executor: Running task 0.1 in stage 12.0 (TID 9)
25/09/19 04:46:12 INFO ShuffleBlockFetcherIterator: Getting 1 (194.1 KiB) non-empty blocks including 1 (194.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
25/09/19 04:46:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
25/09/19 04:46:12 ERROR Executor: Exception in task 0.1 in stage 12.0 (TID 9)
org.apache.iceberg.exceptions.ValidationException: Cannot find source column for partition field: 1000: ingestion_date_day: day(10)
	at org.apache.iceberg.exceptions.ValidationException.check(ValidationException.java:49)
	at org.apache.iceberg.PartitionSpec.checkCompatibility(PartitionSpec.java:562)
	at org.apache.iceberg.PartitionSpec$Builder.build(PartitionSpec.java:543)
	at org.apache.iceberg.UnboundPartitionSpec.bind(UnboundPartitionSpec.java:46)
	at org.apache.iceberg.PartitionSpecParser.fromJson(PartitionSpecParser.java:71)
	at org.apache.iceberg.PartitionSpecParser.lambda$fromJson$1(PartitionSpecParser.java:88)
	at org.apache.iceberg.util.JsonUtil.parse(JsonUtil.java:98)
	at org.apache.iceberg.PartitionSpecParser.lambda$fromJson$2(PartitionSpecParser.java:88)
	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)
	at java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)
	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)
	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)
	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)
	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)
	at org.apache.iceberg.PartitionSpecParser.fromJson(PartitionSpecParser.java:86)
	at org.apache.iceberg.SerializableTable.lambda$specs$1(SerializableTable.java:196)
	at java.base/java.util.HashMap.forEach(HashMap.java:1421)
	at org.apache.iceberg.SerializableTable.specs(SerializableTable.java:194)
	at org.apache.iceberg.spark.source.SparkWrite$WriterFactory.createWriter(SparkWrite.java:674)
	at org.apache.iceberg.spark.source.SparkWrite$WriterFactory.createWriter(SparkWrite.java:668)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:441)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
25/09/19 04:46:12 INFO CoarseGrainedExecutorBackend: Got assigned task 10
25/09/19 04:46:12 INFO Executor: Running task 0.2 in stage 12.0 (TID 10)
25/09/19 04:46:12 INFO ShuffleBlockFetcherIterator: Getting 1 (194.1 KiB) non-empty blocks including 1 (194.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
25/09/19 04:46:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
25/09/19 04:46:12 ERROR Executor: Exception in task 0.2 in stage 12.0 (TID 10)
org.apache.iceberg.exceptions.ValidationException: Cannot find source column for partition field: 1000: ingestion_date_day: day(10)
	at org.apache.iceberg.exceptions.ValidationException.check(ValidationException.java:49)
	at org.apache.iceberg.PartitionSpec.checkCompatibility(PartitionSpec.java:562)
	at org.apache.iceberg.PartitionSpec$Builder.build(PartitionSpec.java:543)
	at org.apache.iceberg.UnboundPartitionSpec.bind(UnboundPartitionSpec.java:46)
	at org.apache.iceberg.PartitionSpecParser.fromJson(PartitionSpecParser.java:71)
	at org.apache.iceberg.PartitionSpecParser.lambda$fromJson$1(PartitionSpecParser.java:88)
	at org.apache.iceberg.util.JsonUtil.parse(JsonUtil.java:98)
	at org.apache.iceberg.PartitionSpecParser.lambda$fromJson$2(PartitionSpecParser.java:88)
	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)
	at java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)
	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)
	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)
	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)
	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)
	at org.apache.iceberg.PartitionSpecParser.fromJson(PartitionSpecParser.java:86)
	at org.apache.iceberg.SerializableTable.lambda$specs$1(SerializableTable.java:196)
	at java.base/java.util.HashMap.forEach(HashMap.java:1421)
	at org.apache.iceberg.SerializableTable.specs(SerializableTable.java:194)
	at org.apache.iceberg.spark.source.SparkWrite$WriterFactory.createWriter(SparkWrite.java:674)
	at org.apache.iceberg.spark.source.SparkWrite$WriterFactory.createWriter(SparkWrite.java:668)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:441)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
25/09/19 04:46:12 INFO CoarseGrainedExecutorBackend: Got assigned task 11
25/09/19 04:46:12 INFO Executor: Running task 0.3 in stage 12.0 (TID 11)
25/09/19 04:46:12 INFO ShuffleBlockFetcherIterator: Getting 1 (194.1 KiB) non-empty blocks including 1 (194.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
25/09/19 04:46:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
25/09/19 04:46:12 ERROR Executor: Exception in task 0.3 in stage 12.0 (TID 11)
org.apache.iceberg.exceptions.ValidationException: Cannot find source column for partition field: 1000: ingestion_date_day: day(10)
	at org.apache.iceberg.exceptions.ValidationException.check(ValidationException.java:49)
	at org.apache.iceberg.PartitionSpec.checkCompatibility(PartitionSpec.java:562)
	at org.apache.iceberg.PartitionSpec$Builder.build(PartitionSpec.java:543)
	at org.apache.iceberg.UnboundPartitionSpec.bind(UnboundPartitionSpec.java:46)
	at org.apache.iceberg.PartitionSpecParser.fromJson(PartitionSpecParser.java:71)
	at org.apache.iceberg.PartitionSpecParser.lambda$fromJson$1(PartitionSpecParser.java:88)
	at org.apache.iceberg.util.JsonUtil.parse(JsonUtil.java:98)
	at org.apache.iceberg.PartitionSpecParser.lambda$fromJson$2(PartitionSpecParser.java:88)
	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)
	at java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)
	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)
	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)
	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)
	at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)
	at org.apache.iceberg.PartitionSpecParser.fromJson(PartitionSpecParser.java:86)
	at org.apache.iceberg.SerializableTable.lambda$specs$1(SerializableTable.java:196)
	at java.base/java.util.HashMap.forEach(HashMap.java:1421)
	at org.apache.iceberg.SerializableTable.specs(SerializableTable.java:194)
	at org.apache.iceberg.spark.source.SparkWrite$WriterFactory.createWriter(SparkWrite.java:674)
	at org.apache.iceberg.spark.source.SparkWrite$WriterFactory.createWriter(SparkWrite.java:668)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:441)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:621)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:624)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
25/09/19 04:46:13 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
25/09/19 04:46:13 ERROR CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
tdown
